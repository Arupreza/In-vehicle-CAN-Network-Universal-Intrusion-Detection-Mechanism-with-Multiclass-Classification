{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9eb037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "NVIDIA GeForce RTX 2080 SUPER\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import pathlib\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec498ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir,makedirs\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATADIR_Train = \"C:/Users/Lisa/Rezanur/Pytorch/MUL_UNI/img set 3/ResNet RGB_T_Tesla/Train\"\n",
    "DATADIR_Test = \"C:/Users/Lisa/Rezanur/Pytorch/MUL_UNI/img set 3/ResNet RGB_T_Tesla/Test\"\n",
    "DATADIR_Val = \"C:/Users/Lisa/Rezanur/Pytorch/MUL_UNI/img set 3/ResNet RGB_T_Tesla/Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7ed70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7458435",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_Train = [\"Normal\", \"DoS\", \"Fuzz\", \"Replay\"]\n",
    "CATEGORIES_Test = [\"Normal\", \"DoS\", \"Fuzz\", \"Replay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44df0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [00:14<00:00, 483.76it/s]\n",
      "100%|██████████| 4223/4223 [00:08<00:00, 488.52it/s]\n",
      "100%|██████████| 10000/10000 [00:20<00:00, 482.79it/s]\n",
      "100%|██████████| 1224/1224 [00:02<00:00, 476.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES_Train:  # do dogs and cats\n",
    "\n",
    "        path = os.path.join(DATADIR_Train,category)  # create path attack\n",
    "        class_num = CATEGORIES_Train.index(category)  # get the classification  (0, 1,.... ).\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image \n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
    "                new_array = cv2.resize(img_array, (100, 100))  # resize to normalize data size\n",
    "                new_array = np.transpose(new_array, (2, 0, 1))\n",
    "                training_data.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "            #except OSError as e:\n",
    "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "            #except Exception as e:\n",
    "            #    print(\"general exception\", e, os.path.join(path,img))\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6ec14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:04<00:00, 470.54it/s]\n",
      "100%|██████████| 3036/3036 [00:06<00:00, 492.75it/s]\n",
      "100%|██████████| 4000/4000 [00:08<00:00, 482.09it/s]\n",
      "100%|██████████| 883/883 [00:01<00:00, 466.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testing_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES_Test:  # do dogs and cats\n",
    "\n",
    "        path = os.path.join(DATADIR_Test,category)  # create path attack\n",
    "        class_num = CATEGORIES_Test.index(category)  # get the classification  (0, 1,.... ).\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image \n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
    "                new_array = cv2.resize(img_array, (100, 100))  # resize to normalize data size\n",
    "                new_array = np.transpose(new_array, (2, 0, 1))\n",
    "                testing_data.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "            #except OSError as e:\n",
    "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "            #except Exception as e:\n",
    "            #    print(\"general exception\", e, os.path.join(path,img))\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "print(len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1097065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [00:09<00:00, 236.27it/s]\n",
      "100%|██████████| 3336/3336 [00:13<00:00, 241.83it/s]\n",
      "100%|██████████| 4300/4300 [00:17<00:00, 239.69it/s]\n",
      "100%|██████████| 1183/1183 [00:05<00:00, 234.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES_Test:  # do dogs and cats\n",
    "\n",
    "        path = os.path.join(DATADIR_Val,category)  # create path attack\n",
    "        class_num = CATEGORIES_Test.index(category)  # get the classification  (0, 1,.... ).\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image \n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
    "                new_array = cv2.resize(img_array, (100, 100))  # resize to normalize data size\n",
    "                new_array = np.transpose(new_array, (2, 0, 1))\n",
    "                val_data.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "            #except OSError as e:\n",
    "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "            #except Exception as e:\n",
    "            #    print(\"general exception\", e, os.path.join(path,img))\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "print(len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f475b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974a381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40001052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
    "    ):\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19853f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(\n",
    "            block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    intermediate_channels * 4,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
    "        )\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f65436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(img_channel=3, num_classes=4):\n",
    "    return ResNet(block, [1, 4, 6, 3], img_channel, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78aba78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, mode, path, patience=3, delta=0):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError(\"Argument mode must be one of 'min' or 'max'.\")\n",
    "        if patience <= 0:\n",
    "            raise ValueError(\"Argument patience must be a postive integer.\")\n",
    "        if delta < 0:\n",
    "            raise ValueError(\"Argument delta must not be a negative number.\")\n",
    "            \n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_score = np.inf if mode == 'min' else -np.inf\n",
    "        self.counter = 0\n",
    "        \n",
    "    def _is_improvement(self, val_score):\n",
    "        \"\"\"Return True iff val_score is better than self.best_score.\"\"\"\n",
    "        if self.mode == 'max' and val_score > self.best_score + self.delta:\n",
    "            return True\n",
    "        elif self.mode == 'min' and val_score < self.best_score - self.delta:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def __call__(self, val_score, model):\n",
    "        \"\"\"Return True iff self.counter >= self.patience.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self._is_improvement(val_score):\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            print('Val loss improved. Saved model.')\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f'Early stopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                print(f'Stopped early. Best val loss: {self.best_score:.4f}')\n",
    "                return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc7060b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, device, criterion):\n",
    "    \"\"\"Train model for one epoch and return the mean train_loss.\"\"\"\n",
    "    model.train()\n",
    "    running_loss_train = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "        #labels = labels.type(torch.cuda.FloatTensor)\n",
    "        inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss_train += loss.item()\n",
    "    train_loss = running_loss_train / len(train_loader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da9592e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_loader, device, criterion):\n",
    "    \"\"\"Validate model and return the accuracy and mean loss.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    running_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "            #labels = labels.type(torch.cuda.FloatTensor)\n",
    "            inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "            running_loss_val += loss.item()\n",
    "    val_acc = correct / len(valid_loader.dataset)\n",
    "    val_loss = running_loss_val / len(valid_loader.dataset)\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2033ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, valid_loader, learning_rate, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    es = EarlyStopping(mode='min', path='./ResNet_Adam_T_Tesla_1.pth', patience=10)\n",
    "    model = model.to(device)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device, criterion)\n",
    "        val_acc, val_loss = validate(model, valid_loader, device, criterion)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch:2}/{num_epochs}',\n",
    "              f'train loss: {train_loss:.4f}',\n",
    "              f'val loss: {val_loss:.4f}',\n",
    "              f'val acc: {val_acc:.2%}',\n",
    "              sep=' | '\n",
    "             )\n",
    "        if es(val_loss, model):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756151f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/50 | train loss: 0.1297 | val loss: 0.2915 | val acc: 88.38%\n",
      "Val loss improved. Saved model.\n",
      "Epoch  2/50 | train loss: 0.0327 | val loss: 0.0869 | val acc: 96.92%\n",
      "Val loss improved. Saved model.\n",
      "Epoch  3/50 | train loss: 0.0216 | val loss: 0.0906 | val acc: 97.10%\n",
      "Early stopping counter: 1/10\n",
      "Epoch  4/50 | train loss: 0.0187 | val loss: 0.0897 | val acc: 97.07%\n",
      "Early stopping counter: 2/10\n",
      "Epoch  5/50 | train loss: 0.0193 | val loss: 0.0894 | val acc: 97.15%\n",
      "Early stopping counter: 3/10\n",
      "Epoch  6/50 | train loss: 0.0189 | val loss: 0.0916 | val acc: 96.92%\n",
      "Early stopping counter: 4/10\n",
      "Epoch  7/50 | train loss: 0.0187 | val loss: 0.0886 | val acc: 97.17%\n",
      "Early stopping counter: 5/10\n",
      "Epoch  8/50 | train loss: 0.0180 | val loss: 0.0870 | val acc: 97.13%\n",
      "Early stopping counter: 6/10\n",
      "Epoch  9/50 | train loss: 0.0195 | val loss: 0.0916 | val acc: 97.01%\n",
      "Early stopping counter: 7/10\n",
      "Epoch 10/50 | train loss: 0.0189 | val loss: 0.0983 | val acc: 96.78%\n",
      "Early stopping counter: 8/10\n",
      "Epoch 11/50 | train loss: 0.0191 | val loss: 0.1024 | val acc: 96.70%\n",
      "Early stopping counter: 9/10\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=VALID_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "model = ResNet50().to(device)\n",
    "start = time.time()\n",
    "fit(model, train_loader, valid_loader, learning_rate=LEARNING_RATE, num_epochs=NUM_EPOCHS)\n",
    "print(f'Total training time: {time.time() - start}')\n",
    "model.load_state_dict(torch.load('ResNet_Adam_T_Tesla_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('C:/Users/Lisa/Rezanur/Pytorch/MUL_UNI/img set 3/ResNet_Adam_T_Tesla_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5342ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = False\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testing_data, batch_size=VALID_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "model = ResNet50()\n",
    "model.load_state_dict(torch.load('C:/Users/Lisa/Rezanur/Pytorch/MUL_UNI/img set 3/ResNet_Adam_T_Tesla_1.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b885583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        prediction = np.zeros(len(dataloader.dataset))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target in dataloader:\n",
    "            if device:\n",
    "                images = images.type(torch.cuda.FloatTensor)\n",
    "            prediction[k:k+len(images)] = np.argmax(model(images).data.cpu().numpy(), axis=1)\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            k += len(images)\n",
    "    return prediction, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ca5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = get_prediction(model, valid_loader)\n",
    "y_pred = pred[0]\n",
    "y_test = pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_names = ['True Neg','False Pos','False Neg','True Pos','True Pos','True Pos','True Pos','True Pos','True Pos']\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "labels = [f\"{v1}\\n{v2}\\n\" for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(4,4)\n",
    "sns.set(rc={'figure.figsize':(15,13)})\n",
    "sns.set(font_scale=1.8) #edited as suggested\n",
    "ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues', cbar=False)\n",
    "\n",
    "\n",
    "ax.set_xlabel('\\nActual Category ', fontname=\"Times New Roman\", size=22,fontweight=\"bold\")\n",
    "ax.set_ylabel('Predicted Category \\n', fontname=\"Times New Roman\", size=22,fontweight=\"bold\");\n",
    "#ax.text(0.30, 4.65, Confusion Matrix CNN Multiclass Classification',fontsize=25,fontweight=\"bold\")\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels([\"Normal\", \"DoS\", \"Fuzz\", \"Replay\"],fontweight=\"bold\", fontname=\"Times New Roman\", size=18)\n",
    "ax.yaxis.set_ticklabels([\"Normal\", \"DoS\", \"Fuzz\", \"Replay\"],fontweight=\"bold\", fontname=\"Times New Roman\", size=18)\n",
    "ax.xaxis.label.set_color('darkblue')\n",
    "ax.yaxis.label.set_color('darkblue')\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb522f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_names = [\"Normal\", \"DoS\", \"Fuzz\", \"Replay\"]\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38567fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "target= [\"Normal\", \"DoS\", \"Fuzz\", \"Replay\"]\n",
    "\n",
    "# set plot figure size\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (15, 10))\n",
    "sns.set(font_scale=1.5) #edited as suggested\n",
    "\n",
    "# function for scoring roc auc score for multi-class\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    for (idx, c_label) in enumerate(target):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(y_test, y_pred))\n",
    "\n",
    "c_ax.set_xlabel('False Positive Rate(TPR)', fontname=\"Times New Roman\", size=18, fontweight=\"bold\", color='darkblue')\n",
    "c_ax.set_ylabel('True Positive Rate (FPR)', fontname=\"Times New Roman\", size=18, fontweight=\"bold\", color='darkblue')\n",
    "#c_ax.text(0.16, -0.22, 'Fig. 6. (c) RUC Score CNN Multiclass Class Classification', fontname=\"Times New Roman\",fontsize=25,fontweight=\"bold\")\n",
    "c_ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1790c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f61d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b78b632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
